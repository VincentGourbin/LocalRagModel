---
title: Swift MLX documentation research
emoji: üîç
colorFrom: blue
colorTo: green
sdk: gradio
sdk_version: 5.43.1
app_file: step03_chatbot.py
pinned: false
license: mit
hardware: zerogpu
short_description: Search in the Swift MLX documentation
models:
- Qwen/Qwen3-Embedding-4B
- Qwen/Qwen3-Reranker-4B
- Qwen/Qwen3-4B-Instruct-2507
datasets:
- VincentGOURBIN/swift-mlx-Qwen3-Embedding-4B
tags:
- rag
- retrieval-augmented-generation
- qwen3
- semantic-search
- question-answering
- zero-gpu
- mcp-server
- faiss
---

# üîç LocalRAG - Syst√®me RAG Complet avec Qwen3

‚úÖ **Syst√®me complet et fonctionnel** - Compatible MPS (Mac), CUDA et ZeroGPU Spaces

Syst√®me RAG (Retrieval-Augmented Generation) utilisant les mod√®les Qwen3 de derni√®re g√©n√©ration avec pipeline 2 √©tapes : recherche vectorielle + reranking + g√©n√©ration stream√©e.

## ‚ö° Fonctionnalit√©s

### üß† **Mod√®les IA Avanc√©s**
- **Embeddings**: Qwen3-Embedding-4B (2560 dimensions)
- **Reranking**: Qwen3-Reranker-4B pour l'affinage des r√©sultats
- **G√©n√©ration**: Qwen3-4B-Instruct-2507 avec streaming
- **Optimisation ZeroGPU**: Support natif avec d√©corateurs @spaces.GPU

### üîç **Recherche S√©mantique Avanc√©e**
- **Pipeline 2 √©tapes**: Recherche vectorielle + reranking
- **Index FAISS**: Recherche haute performance sur de gros volumes
- **Scores d√©taill√©s**: Embedding + reranking pour chaque document
- **S√©lection intelligente**: Top-K adaptatif selon pertinence

### üí¨ **G√©n√©ration Contextuelle**
- **Streaming**: R√©ponse progressive token par token
- **Contexte enrichi**: Int√©gration des documents les plus pertinents
- **R√©f√©rences**: Sources avec scores de pertinence
- **Qualit√©**: R√©ponses bas√©es uniquement sur le contexte fourni

### üîå **Int√©gration MCP**
- **Serveur MCP natif**: Fonction `ask_rag_question()` expos√©e
- **Param√®tres configurables**: Nombre documents, activation reranking
- **Compatible**: Claude Desktop, VS Code, Cursor IDE
- **API structur√©e**: R√©ponses JSON avec sources et m√©tadonn√©es

## üõ†Ô∏è Architecture & √âtapes

### Pipeline de Traitement
1. **Step01** : Indexation des documents (FAISS + m√©tadonn√©es)
2. **Step02** : Upload embeddings vers HuggingFace Hub
3. **Step03** : Interface de chat RAG (compatible MPS/CUDA/ZeroGPU)
4. **Step04** : D√©ploiement automatique sur HuggingFace Spaces

### Recherche en 2 √âtapes
1. **Recherche vectorielle** : FAISS IndexFlatIP (cosine similarity)
2. **Reranking** : Qwen3-Reranker-4B pour affiner la pertinence
3. **G√©n√©ration** : Qwen3-4B-Instruct-2507 avec streaming

## üöÄ Utilisation

### Interface Web
1. **Posez votre question** dans le chat
2. **Observez la recherche** en 2 √©tapes (vectorielle ‚Üí reranking)
3. **Lisez la r√©ponse** g√©n√©r√©e en streaming
4. **Consultez les sources** avec scores de pertinence

### Installation Locale
```bash
# Cloner le projet
git clone [repo-url]
cd LocalRagModel

# Installer les d√©pendances
pip install -r requirements.txt

# Lancer l'interface
python step03_chatbot.py
```

### Configuration des Mod√®les
- **MPS (Mac)** : Support natif, Flash Attention d√©sactiv√©
- **CUDA (NVIDIA)** : Flash Attention disponible (d√©sactivable)  
- **ZeroGPU** : D√©corateurs `@spaces.GPU` automatiques

### Int√©gration MCP
Connectez votre client MCP pour un acc√®s programmatique :
```python
# Exemple d'utilisation MCP
result = mcp_client.call_tool(
    "ask_rag_question",
    question="Comment impl√©menter des r√©seaux de neurones complexes?",
    num_documents=3,
    use_reranking=True
)
```

## üéØ Cas d'Usage Parfaits

- **Documentation technique**: Recherche dans APIs, guides, tutoriels
- **Support client**: R√©ponses bas√©es sur une base de connaissances
- **Recherche acad√©mique**: Analyse de corpus documentaires
- **Assistance d√©veloppeur**: Aide contextuelle sur frameworks/librairies
- **Formation**: Syst√®me de questions-r√©ponses intelligent

## üìä Performance

- **Recherche**: ~50ms pour 10K+ documents
- **Reranking**: ~200ms pour 20 candidats  
- **G√©n√©ration**: ~2-4s avec streaming
- **M√©moire**: ~6-8GB optimis√© pour ZeroGPU

## üîí S√©curit√© & Confidentialit√©

- **ZeroGPU**: Traitement s√©curis√© sans stockage persistant
- **Donn√©es temporaires**: Pas de r√©tention des questions/r√©ponses
- **Mod√®les locaux**: Traitement dans l'environnement HF Spaces

## üìö Source des Donn√©es

Ce Space utilise des embeddings pr√©-calcul√©s depuis le dataset :
**[VincentGOURBIN/swift-mlx-Qwen3-Embedding-4B](https://huggingface.co/datasets/VincentGOURBIN/swift-mlx-Qwen3-Embedding-4B)**

## üìã TODO / Roadmap

### ‚úÖ Fonctionnalit√©s Compl√®tes
- [x] Pipeline RAG complet (recherche + reranking + g√©n√©ration)
- [x] Interface Gradio avec streaming
- [x] Support multi-plateforme (MPS/CUDA/ZeroGPU)
- [x] Int√©gration MCP native
- [x] D√©ploiement automatique HuggingFace Spaces

### üîÑ Am√©liorations Futures
- [ ] **Optimisation performances** (caching, batch processing)
- [ ] **Upload documents sources** vers HuggingFace Hub
- [ ] **Step00** : T√©l√©chargement automatique de documentation technique depuis internet
- [ ] Support formats additionnels (PDF, DOCX, HTML)
- [ ] Interface d'administration pour gestion des documents

---

üöÄ **Projet complet et fonctionnel** - Commencez √† poser vos questions pour d√©couvrir la puissance du RAG avec Qwen3! üîç‚ú®
