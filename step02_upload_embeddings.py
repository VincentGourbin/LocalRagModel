#!/usr/bin/env python3
"""
Step 02 - Upload des embeddings vers Hugging Face
Convertit les index FAISS en format SafeTensors et upload vers HF Hub
"""

import os
import json
import pickle
import getpass
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime
import argparse
import sys

def check_dependencies():
    """VÃ©rifie les dÃ©pendances nÃ©cessaires."""
    missing = []
    try:
        import torch
    except ImportError:
        missing.append("torch")
    
    try:
        import numpy as np
    except ImportError:
        missing.append("numpy")
        
    try:
        from safetensors.torch import save_file, load_file
    except ImportError:
        missing.append("safetensors")
        
    try:
        from huggingface_hub import HfApi, create_repo, upload_file
    except ImportError:
        missing.append("huggingface-hub")
        
    try:
        import faiss
    except ImportError:
        missing.append("faiss-cpu")
    
    if missing:
        print(f"âŒ DÃ©pendances manquantes: {', '.join(missing)}")
        print("ğŸ“¦ Installer avec: pip install " + " ".join(missing))
        return False
    return True


class EmbeddingUploader:
    """Gestionnaire d'upload des embeddings vers Hugging Face Hub."""
    
    def __init__(self, hf_token: str):
        """
        Initialise l'uploader avec le token Hugging Face.
        
        Args:
            hf_token: Token d'accÃ¨s Hugging Face
        """
        from huggingface_hub import HfApi
        
        self.hf_token = hf_token
        self.api = HfApi(token=hf_token)
        
    def convert_faiss_to_safetensors(self, faiss_index_path: str) -> Tuple[Dict, Dict]:
        """
        Convertit un index FAISS en format SafeTensors.
        
        Args:
            faiss_index_path: Chemin vers le rÃ©pertoire d'index FAISS
            
        Returns:
            Tuple[Dict, Dict]: (tensors_dict, metadata_dict)
        """
        # Imports locaux pour Ã©viter les erreurs si non installÃ©
        import torch
        import numpy as np
        import faiss
        from safetensors.torch import save_file
        
        print(f"ğŸ”„ Conversion FAISS â†’ SafeTensors depuis {faiss_index_path}")
        
        index_dir = Path(faiss_index_path)
        
        # Charger l'index FAISS
        index_file = index_dir / "index.faiss"
        metadata_file = index_dir / "metadata.json"
        mappings_file = index_dir / "mappings.pkl"
        
        if not index_file.exists():
            raise FileNotFoundError(f"Index FAISS non trouvÃ©: {index_file}")
        
        print("  ğŸ“‚ Chargement index FAISS...")
        faiss_index = faiss.read_index(str(index_file))
        
        # Extraire les vecteurs depuis FAISS
        print("  ğŸ“Š Extraction des vecteurs...")
        n_vectors = faiss_index.ntotal
        dimension = faiss_index.d
        
        print(f"     Vecteurs: {n_vectors:,}")
        print(f"     Dimension: {dimension}")
        
        # Reconstruction des vecteurs depuis l'index FAISS
        vectors = np.zeros((n_vectors, dimension), dtype=np.float32)
        
        if n_vectors > 0:
            # Pour les index HNSW, on doit reconstruire les vecteurs
            if hasattr(faiss_index, 'storage'):
                # Index avec storage (comme HNSW)
                storage = faiss_index.storage
                for i in range(n_vectors):
                    vectors[i] = storage.reconstruct(i)
            else:
                # Index flat
                vectors = faiss_index.reconstruct_n(0, n_vectors)
        
        # Charger les mÃ©tadonnÃ©es JSON
        metadata_dict = {}
        if metadata_file.exists():
            print("  ğŸ“‹ Chargement mÃ©tadonnÃ©es...")
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata_dict = json.load(f)
        
        # Charger les mappings
        id_to_idx = {}
        idx_to_id = {}
        if mappings_file.exists():
            print("  ğŸ”— Chargement mappings...")
            with open(mappings_file, 'rb') as f:
                mappings = pickle.load(f)
                id_to_idx = mappings.get('id_to_idx', {})
                idx_to_id = mappings.get('idx_to_id', {})
        
        # CrÃ©er les tensors pour SafeTensors
        tensors_dict = {
            'embeddings': torch.from_numpy(vectors),
        }
        
        # Convertir les mappings en format compatible SafeTensors
        # CrÃ©er des tenseurs pour les IDs (en utilisant les hash des strings)
        if id_to_idx:
            print("  ğŸ”§ Conversion mappings...")
            
            # CrÃ©er une liste ordonnÃ©e des IDs par index
            ordered_ids = [''] * n_vectors
            for id_str, idx in id_to_idx.items():
                if idx < n_vectors:
                    ordered_ids[idx] = id_str
            
            # Encoder les IDs comme strings dans les mÃ©tadonnÃ©es
            metadata_dict['ordered_ids'] = ordered_ids
            metadata_dict['id_to_idx'] = id_to_idx
        
        # Ajouter des mÃ©tadonnÃ©es techniques
        metadata_dict.update({
            'format_version': '1.0',
            'total_vectors': n_vectors,
            'vector_dimension': dimension,
            'faiss_index_type': 'HNSW',
            'conversion_timestamp': datetime.now().isoformat(),
            'embedding_model': 'Qwen/Qwen3-Embedding-4B'  # Ã€ adapter selon config
        })
        
        print("âœ… Conversion terminÃ©e")
        print(f"  ğŸ“Š Tenseurs: {list(tensors_dict.keys())}")
        print(f"  ğŸ“‹ MÃ©tadonnÃ©es: {len(metadata_dict)} entrÃ©es")
        
        return tensors_dict, metadata_dict
    
    def upload_to_huggingface(self, 
                             tensors_dict: Dict,
                             metadata_dict: Dict,
                             repo_name: str,
                             dataset_name: str = "embeddings",
                             private: bool = True) -> str:
        """
        Upload les embeddings vers Hugging Face Hub.
        
        Args:
            tensors_dict: Dictionnaire des tenseurs
            metadata_dict: MÃ©tadonnÃ©es enrichies
            repo_name: Nom du repository (format: username/repo-name)
            dataset_name: Nom du dataset (dÃ©faut: embeddings)
            private: Repository privÃ© ou public
            
        Returns:
            URL du repository crÃ©Ã©
        """
        # Imports locaux pour HF Hub
        from safetensors.torch import save_file
        from huggingface_hub import create_repo, upload_file
        
        print(f"ğŸš€ Upload vers Hugging Face Hub: {repo_name}")
        
        try:
            # CrÃ©er le repository s'il n'existe pas
            print("  ğŸ“¦ CrÃ©ation/vÃ©rification repository...")
            repo_url = create_repo(
                repo_id=repo_name,
                token=self.hf_token,
                private=private,
                exist_ok=True,
                repo_type="dataset"
            )
            print(f"  âœ… Repository: {repo_url}")
            
            # CrÃ©er des fichiers temporaires pour l'upload
            temp_dir = Path("./temp_upload")
            temp_dir.mkdir(exist_ok=True)
            
            try:
                # Sauvegarder les tenseurs en SafeTensors
                print("  ğŸ’¾ Sauvegarde SafeTensors...")
                safetensors_file = temp_dir / f"{dataset_name}.safetensors"
                save_file(tensors_dict, str(safetensors_file))
                
                # Sauvegarder les mÃ©tadonnÃ©es en JSON
                print("  ğŸ’¾ Sauvegarde mÃ©tadonnÃ©es...")
                metadata_file = temp_dir / f"{dataset_name}_metadata.json"
                with open(metadata_file, 'w', encoding='utf-8') as f:
                    json.dump(metadata_dict, f, ensure_ascii=False, indent=2)
                
                # CrÃ©er un README pour le dataset
                print("  ğŸ“ GÃ©nÃ©ration README...")
                readme_file = temp_dir / "README.md"
                self._create_dataset_readme(readme_file, repo_name, dataset_name, metadata_dict)
                
                # Upload des fichiers
                print("  ğŸ“¤ Upload fichiers...")
                
                files_to_upload = [
                    (safetensors_file, f"{dataset_name}.safetensors"),
                    (metadata_file, f"{dataset_name}_metadata.json"),
                    (readme_file, "README.md")
                ]
                
                for local_file, remote_path in files_to_upload:
                    print(f"    ğŸ“¤ {remote_path}...")
                    upload_file(
                        path_or_fileobj=str(local_file),
                        path_in_repo=remote_path,
                        repo_id=repo_name,
                        repo_type="dataset",
                        token=self.hf_token
                    )
                
                print("âœ… Upload terminÃ© avec succÃ¨s !")
                
                # Informations de tÃ©lÃ©chargement
                print(f"\nğŸ“‹ Informations du dataset:")
                print(f"  ğŸ”— URL: https://huggingface.co/datasets/{repo_name}")
                print(f"  ğŸ“Š Vecteurs: {metadata_dict.get('total_vectors', 'N/A'):,}")
                print(f"  ğŸ“ Dimension: {metadata_dict.get('vector_dimension', 'N/A')}")
                print(f"  ğŸ’¾ Taille SafeTensors: {safetensors_file.stat().st_size / 1024 / 1024:.1f} MB")
                
                # Sauvegarder la configuration pour Step 03
                self._save_step03_config(repo_name, dataset_name, metadata_dict)
                
                return repo_url
                
            finally:
                # Nettoyage des fichiers temporaires
                import shutil
                shutil.rmtree(temp_dir, ignore_errors=True)
                
        except Exception as e:
            print(f"âŒ Erreur lors de l'upload: {e}")
            raise
    
    def _create_dataset_readme(self, readme_file: Path, repo_name: str, dataset_name: str, metadata_dict: Dict):
        """CrÃ©er un README descriptif pour le dataset."""
        content = f"""---
title: {repo_name}
emoji: ğŸ”
colorFrom: blue
colorTo: green
sdk: static
app_file: README.md
pinned: false
license: apache-2.0
tags:
- embeddings
- faiss
- rag
- semantic-search
- safetensors
---

# ğŸ” {repo_name} - Embeddings Dataset

## Description

Ce dataset contient des embeddings vectoriels gÃ©nÃ©rÃ©s par le systÃ¨me LocalRAG pour la recherche sÃ©mantique dans la documentation technique.

## ğŸ“Š Statistiques

- **Format**: SafeTensors
- **Vecteurs**: {metadata_dict.get('total_vectors', 'N/A'):,}
- **Dimension**: {metadata_dict.get('vector_dimension', 'N/A')}
- **ModÃ¨le d'embedding**: {metadata_dict.get('embedding_model', 'N/A')}
- **Type d'index**: {metadata_dict.get('faiss_index_type', 'N/A')}
- **GÃ©nÃ©rÃ© le**: {metadata_dict.get('conversion_timestamp', 'N/A')}

## ğŸ“ Contenu

- `{dataset_name}.safetensors`: Embeddings vectoriels au format SafeTensors
- `{dataset_name}_metadata.json`: MÃ©tadonnÃ©es complÃ¨tes avec mappings
- `README.md`: Cette documentation

## ğŸš€ Utilisation

### Chargement avec Hugging Face Hub

```python
from huggingface_hub import hf_hub_download
from safetensors.torch import load_file
import json

# TÃ©lÃ©charger les fichiers
embeddings_file = hf_hub_download(repo_id="{repo_name}", filename="{dataset_name}.safetensors")
metadata_file = hf_hub_download(repo_id="{repo_name}", filename="{dataset_name}_metadata.json")

# Charger les embeddings
tensors = load_file(embeddings_file)
embeddings = tensors['embeddings']  # Shape: [n_vectors, dimension]

# Charger les mÃ©tadonnÃ©es
with open(metadata_file, 'r') as f:
    metadata = json.load(f)

print(f"Embeddings shape: {{embeddings.shape}}")
print(f"Total vectors: {{metadata['total_vectors']}}")
```

### Recherche sÃ©mantique

```python
import torch
import torch.nn.functional as F

def semantic_search(query_embedding, embeddings, top_k=10):
    \"\"\"Recherche sÃ©mantique dans les embeddings.\"\"\"
    # Calcul de similaritÃ© cosinus
    similarities = F.cosine_similarity(query_embedding.unsqueeze(0), embeddings, dim=1)
    
    # Top-K rÃ©sultats
    top_scores, top_indices = torch.topk(similarities, top_k)
    
    return top_indices, top_scores

# Exemple d'utilisation
query_emb = torch.randn(1, {metadata_dict.get('vector_dimension', 'dimension')})  # Votre embedding de requÃªte
indices, scores = semantic_search(query_emb, embeddings)
```

## ğŸ”§ GÃ©nÃ©rÃ© par

Ce dataset a Ã©tÃ© gÃ©nÃ©rÃ© par [LocalRAG](https://github.com/your-repo/LocalRAG), un systÃ¨me RAG local complet pour la documentation technique.

- **Step 01**: Indexation vectorielle avec FAISS
- **Step 02**: Conversion SafeTensors et upload HF Hub
- **Step 03**: Recherche sÃ©mantique (Ã  venir)
- **Step 04**: GÃ©nÃ©ration RAG (Ã  venir)

## ğŸ“ License

Apache 2.0 - Voir LICENSE pour plus de dÃ©tails.
"""
        
        with open(readme_file, 'w', encoding='utf-8') as f:
            f.write(content)
    
    def _save_step03_config(self, repo_name: str, dataset_name: str, metadata_dict: Dict):
        """Sauvegarde la configuration pour Step 03."""
        config_file = Path("step03_config.json")
        
        # Configuration complÃ¨te pour Step 03
        step03_config = {
            "step02_completed": True,
            "completion_timestamp": datetime.now().isoformat(),
            "huggingface": {
                "repo_id": repo_name,
                "dataset_name": dataset_name,
                "repo_type": "dataset",
                "files": {
                    "embeddings": f"{dataset_name}.safetensors",
                    "metadata": f"{dataset_name}_metadata.json",
                    "readme": "README.md"
                }
            },
            "embeddings_info": {
                "total_vectors": metadata_dict.get("total_vectors", 0),
                "vector_dimension": metadata_dict.get("vector_dimension", 0),
                "embedding_model": metadata_dict.get("embedding_model", "unknown"),
                "faiss_index_type": metadata_dict.get("faiss_index_type", "HNSW"),
                "format_version": metadata_dict.get("format_version", "1.0")
            },
            "usage_examples": {
                "download_command": f'hf_hub_download(repo_id="{repo_name}", filename="{dataset_name}.safetensors")',
                "load_command": f'tensors = load_file("{dataset_name}.safetensors"); embeddings = tensors["embeddings"]'
            }
        }
        
        # Sauvegarder avec formatage lisible
        try:
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(step03_config, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ’¾ Configuration Step 03 sauvegardÃ©e: {config_file}")
            print(f"  ğŸ“¦ Repository HF: {repo_name}")
            print(f"  ğŸ“Š Embeddings: {metadata_dict.get('total_vectors', 0):,} vecteurs")
            print(f"  ğŸ“ Dimension: {metadata_dict.get('vector_dimension', 0)}")
            print(f"  ğŸ”§ PrÃªt pour Step 03 !")
            
        except Exception as e:
            print(f"âš ï¸ Erreur sauvegarde config Step 03: {e}")
            print(f"   Repository: {repo_name} (Ã  noter manuellement)")
    
    def _create_dataset_readme(self, readme_file: Path, repo_name: str, dataset_name: str, metadata_dict: Dict):
        """CrÃ©er un README descriptif pour le dataset."""
        content = f"""---
title: {repo_name}
emoji: ğŸ”
colorFrom: blue
colorTo: green
sdk: static
app_file: README.md
pinned: false
license: apache-2.0
tags:
- embeddings
- faiss
- rag
- semantic-search
- safetensors
---

# ğŸ” {repo_name} - Embeddings Dataset

## Description

Ce dataset contient des embeddings vectoriels gÃ©nÃ©rÃ©s par le systÃ¨me LocalRAG pour la recherche sÃ©mantique dans la documentation technique.

## ğŸ“Š Statistiques

- **Format**: SafeTensors
- **Vecteurs**: {metadata_dict.get('total_vectors', 'N/A'):,}
- **Dimension**: {metadata_dict.get('vector_dimension', 'N/A')}
- **ModÃ¨le d'embedding**: {metadata_dict.get('embedding_model', 'N/A')}
- **Type d'index**: {metadata_dict.get('faiss_index_type', 'N/A')}
- **GÃ©nÃ©rÃ© le**: {metadata_dict.get('conversion_timestamp', 'N/A')}

## ğŸ“ Contenu

- `{dataset_name}.safetensors`: Embeddings vectoriels au format SafeTensors
- `{dataset_name}_metadata.json`: MÃ©tadonnÃ©es complÃ¨tes avec mappings
- `README.md`: Cette documentation

## ğŸš€ Utilisation

### Chargement avec Hugging Face Hub

```python
from huggingface_hub import hf_hub_download
from safetensors.torch import load_file
import json

# TÃ©lÃ©charger les fichiers
embeddings_file = hf_hub_download(repo_id="{repo_name}", filename="{dataset_name}.safetensors")
metadata_file = hf_hub_download(repo_id="{repo_name}", filename="{dataset_name}_metadata.json")

# Charger les embeddings
tensors = load_file(embeddings_file)
embeddings = tensors['embeddings']  # Shape: [n_vectors, dimension]

# Charger les mÃ©tadonnÃ©es
with open(metadata_file, 'r') as f:
    metadata = json.load(f)

print(f"Embeddings shape: {{embeddings.shape}}")
print(f"Total vectors: {{metadata['total_vectors']}}")
```

## ğŸ“ License

Apache 2.0 - Voir LICENSE pour plus de dÃ©tails.
"""
        
        with open(readme_file, 'w', encoding='utf-8') as f:
            f.write(content)


def get_user_inputs() -> Tuple[str, str, str, bool]:
    """Collecte les inputs utilisateur de maniÃ¨re interactive."""
    print("ğŸ”‘ Configuration Hugging Face Hub")
    print("=" * 40)
    
    # Token HF
    token = getpass.getpass("ğŸ” Token Hugging Face (masquÃ©): ")
    if not token.strip():
        raise ValueError("Token Hugging Face requis")
    
    # Nom du repository
    print("\nğŸ“¦ Configuration du repository")
    repo_name = input("ğŸ“› Nom du repository (format: username/repo-name): ").strip()
    if not repo_name or '/' not in repo_name:
        raise ValueError("Format repository invalide (doit contenir username/repo-name)")
    
    # Nom du dataset
    dataset_name = input("ğŸ“Š Nom du dataset [embeddings]: ").strip() or "embeddings"
    
    # Repository privÃ©
    private_input = input("ğŸ”’ Repository privÃ© ? (y/N): ").strip().lower()
    private = private_input in ['y', 'yes', 'oui']
    
    return token, repo_name, dataset_name, private


def find_faiss_indexes() -> List[Path]:
    """Trouve tous les rÃ©pertoires d'index FAISS disponibles."""
    current_dir = Path(".")
    faiss_dirs = []
    
    # Chercher tous les dossiers contenant un index.faiss
    for item in current_dir.iterdir():
        if item.is_dir() and "faiss" in item.name.lower():
            index_file = item / "index.faiss"
            if index_file.exists():
                faiss_dirs.append(item)
    
    return sorted(faiss_dirs)

def select_faiss_index(provided_path: str = None) -> Path:
    """SÃ©lectionne l'index FAISS Ã  utiliser."""
    if provided_path:
        faiss_path = Path(provided_path)
        if faiss_path.exists() and (faiss_path / "index.faiss").exists():
            return faiss_path
        else:
            print(f"âš ï¸ Chemin fourni invalide: {faiss_path}")
    
    # Auto-dÃ©tection des index disponibles
    available_indexes = find_faiss_indexes()
    
    if not available_indexes:
        print("âŒ Aucun index FAISS trouvÃ© dans le rÃ©pertoire courant")
        print("ğŸ’¡ Lancez d'abord: python step01_indexer.py /path/to/docs")
        sys.exit(1)
    
    if len(available_indexes) == 1:
        selected = available_indexes[0]
        print(f"ğŸ¯ Index FAISS auto-dÃ©tectÃ©: {selected}")
        return selected
    
    # Plusieurs index disponibles, demander Ã  l'utilisateur
    print(f"ğŸ“‚ {len(available_indexes)} index FAISS trouvÃ©s:")
    for i, idx_path in enumerate(available_indexes):
        # Lire quelques stats de l'index si possible
        try:
            metadata_file = idx_path / "metadata.json"
            if metadata_file.exists():
                with open(metadata_file, 'r') as f:
                    metadata = json.load(f)
                    count = len(metadata)
                    print(f"  {i+1}. {idx_path} ({count:,} documents)")
            else:
                print(f"  {i+1}. {idx_path}")
        except:
            print(f"  {i+1}. {idx_path}")
    
    while True:
        try:
            choice = input(f"\nChoisissez un index (1-{len(available_indexes)}): ").strip()
            idx = int(choice) - 1
            if 0 <= idx < len(available_indexes):
                selected = available_indexes[idx]
                print(f"âœ… Index sÃ©lectionnÃ©: {selected}")
                return selected
            else:
                print(f"âŒ Choix invalide. Entrez un nombre entre 1 et {len(available_indexes)}")
        except ValueError:
            print("âŒ Veuillez entrer un nombre valide")
        except KeyboardInterrupt:
            print("\nâ¹ï¸ AnnulÃ© par l'utilisateur")
            sys.exit(1)

def main():
    """Point d'entrÃ©e principal."""
    parser = argparse.ArgumentParser(description="Upload des embeddings FAISS vers Hugging Face Hub")
    parser.add_argument("faiss_index_path", nargs="?", 
                       help="Chemin vers l'index FAISS (auto-dÃ©tection si non spÃ©cifiÃ©)")
    parser.add_argument("--token", help="Token Hugging Face (ou utiliser input interactif)")
    parser.add_argument("--repo-name", help="Nom du repository (format: username/repo-name)")
    parser.add_argument("--dataset-name", default="embeddings", help="Nom du dataset")
    parser.add_argument("--private", action="store_true", help="Repository privÃ©")
    parser.add_argument("--dry-run", action="store_true", help="Test de conversion sans upload")
    
    args = parser.parse_args()
    
    print("ğŸš€ LocalRAG Step 02 - Upload des embeddings")
    print("=" * 50)
    
    # VÃ©rification des dÃ©pendances seulement si pas --help
    if '--help' not in sys.argv and '-h' not in sys.argv:
        if not check_dependencies():
            sys.exit(1)
    
    # SÃ©lection de l'index FAISS
    faiss_path = select_faiss_index(args.faiss_index_path)
    print(f"âœ… Index FAISS confirmÃ©: {faiss_path}")
    
    try:
        # Conversion FAISS â†’ SafeTensors
        print(f"\nğŸ“‹ Ã‰tape 1/3: Conversion")
        uploader = EmbeddingUploader("dummy_token")  # Token temporaire pour conversion
        tensors_dict, metadata_dict = uploader.convert_faiss_to_safetensors(str(faiss_path))
        
        if args.dry_run:
            print("\nâœ… Test de conversion rÃ©ussi (mode --dry-run)")
            print(f"ğŸ“Š Vecteurs convertis: {metadata_dict['total_vectors']:,}")
            print(f"ğŸ“ Dimension: {metadata_dict['vector_dimension']}")
            return
        
        # Collecte des inputs utilisateur
        if args.token and args.repo_name:
            token, repo_name, dataset_name, private = args.token, args.repo_name, args.dataset_name, args.private
        else:
            print(f"\nğŸ“‹ Ã‰tape 2/3: Configuration")
            token, repo_name, dataset_name, private = get_user_inputs()
        
        # Upload vers HF Hub
        print(f"\nğŸ“‹ Ã‰tape 3/3: Upload")
        uploader = EmbeddingUploader(token)
        repo_url = uploader.upload_to_huggingface(
            tensors_dict, metadata_dict, repo_name, dataset_name, private
        )
        
        print("\nğŸ‰ Upload terminÃ© avec succÃ¨s !")
        print(f"ğŸ”— Repository: https://huggingface.co/datasets/{repo_name}")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ OpÃ©ration annulÃ©e par l'utilisateur")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ Erreur: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()