#!/usr/bin/env python3
"""
Step 04 - D√©ploiement HuggingFace Spaces pour LocalRAG
D√©ploie le syst√®me RAG complet vers HuggingFace Spaces avec support ZeroGPU et MCP
"""

import os
import sys
import subprocess
import shutil
from pathlib import Path
import tempfile
import getpass
import json
import argparse

def check_dependencies():
    """V√©rifie les d√©pendances n√©cessaires"""
    print("üîç V√©rification des d√©pendances...")
    
    # V√©rifier git
    try:
        subprocess.run(["git", "--version"], check=True, capture_output=True)
        print("‚úÖ Git install√©")
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("‚ùå Git non install√©. Veuillez installer git d'abord.")
        sys.exit(1)
    
    # V√©rifier huggingface_hub
    try:
        import huggingface_hub
        print("‚úÖ HuggingFace Hub disponible")
    except ImportError:
        print("‚ùå HuggingFace Hub non trouv√©. Installation...")
        subprocess.run([sys.executable, "-m", "pip", "install", "huggingface_hub"], check=True)
        print("‚úÖ HuggingFace Hub install√©")

def get_hf_token():
    """R√©cup√®re le token HuggingFace depuis l'environnement ou saisie utilisateur"""
    token = os.getenv("HF_TOKEN") or os.getenv("HUGGINGFACE_HUB_TOKEN")
    
    if not token:
        print("\nüîë Token HuggingFace requis pour le d√©ploiement")
        print("Obtenez votre token sur: https://huggingface.co/settings/tokens")
        print("Assurez-vous que votre token a les permissions 'write'")
        token = getpass.getpass("Entrez votre token HuggingFace: ").strip()
    
    if not token:
        print("‚ùå Aucun token fourni. D√©ploiement annul√©.")
        sys.exit(1)
    
    return token

def get_space_info():
    """R√©cup√®re les informations du Space aupr√®s de l'utilisateur"""
    print("\nüìù Configuration du Space HuggingFace")
    print("-" * 40)
    
    # Nom du space (format username/space-name)
    space_name = input("Nom du Space (ex: username/localrag-demo): ").strip()
    if not space_name or '/' not in space_name:
        print("‚ùå Format invalide. Utilisez: username/space-name")
        sys.exit(1)
    
    # Titre du space
    default_title = "üîç LocalRAG - RAG System with Qwen3"
    space_title = input(f"Titre du Space (d√©faut: {default_title}): ").strip()
    if not space_title:
        space_title = default_title
    
    # Description courte
    default_desc = "Local RAG system with Qwen3 models and reranking"
    space_desc = input(f"Description courte (d√©faut: {default_desc}): ").strip()
    if not space_desc:
        space_desc = default_desc
    
    # Repository d'embeddings depuis step03_config.json
    config_file = Path("step03_config.json")
    repo_id = None
    if config_file.exists():
        try:
            with open(config_file, 'r') as f:
                config = json.load(f)
                repo_id = config.get('huggingface', {}).get('repo_id')
                if repo_id:
                    print(f"üì¶ Repository embeddings d√©tect√©: {repo_id}")
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lecture step03_config.json: {e}")
    
    if not repo_id:
        repo_id = input("Repository HF des embeddings (ex: username/embeddings): ").strip()
        if not repo_id:
            print("‚ùå Repository embeddings requis")
            sys.exit(1)
    
    # Visibilit√© du space
    is_private = input("Space priv√©? (y/N): ").strip().lower() in ['y', 'yes', 'oui']
    
    return {
        'name': space_name,
        'title': space_title,
        'description': space_desc,
        'repo_id': repo_id,
        'private': is_private
    }


def create_space_readme(space_info):
    """Cr√©e le README.md pour le Space"""
    print("üìù Cr√©ation du README.md pour le Space...")
    
    readme_content = f'''---
title: {space_info['title']}
emoji: üîç
colorFrom: blue
colorTo: green
sdk: gradio
sdk_version: 5.43.1
app_file: step03_chatbot.py
pinned: false
license: mit
hardware: zerogpu
short_description: {space_info['description']}
models:
- Qwen/Qwen3-Embedding-4B
- Qwen/Qwen3-Reranker-4B
- Qwen/Qwen3-4B-Instruct-2507
datasets:
- {space_info['repo_id']}
tags:
- rag
- retrieval-augmented-generation
- qwen3
- semantic-search
- question-answering
- zero-gpu
- mcp-server
- faiss
---

# üîç LocalRAG - Syst√®me RAG Complet avec Qwen3

Syst√®me RAG (Retrieval-Augmented Generation) complet utilisant les mod√®les Qwen3 de derni√®re g√©n√©ration avec reranking et g√©n√©ration stream√©e.

## ‚ö° Fonctionnalit√©s

### üß† **Mod√®les IA Avanc√©s**
- **Embeddings**: Qwen3-Embedding-4B (2560 dimensions)
- **Reranking**: Qwen3-Reranker-4B pour l'affinage des r√©sultats
- **G√©n√©ration**: Qwen3-4B-Instruct-2507 avec streaming
- **Optimisation ZeroGPU**: Support natif avec d√©corateurs @spaces.GPU

### üîç **Recherche S√©mantique Avanc√©e**
- **Pipeline 2 √©tapes**: Recherche vectorielle + reranking
- **Index FAISS**: Recherche haute performance sur de gros volumes
- **Scores d√©taill√©s**: Embedding + reranking pour chaque document
- **S√©lection intelligente**: Top-K adaptatif selon pertinence

### üí¨ **G√©n√©ration Contextuelle**
- **Streaming**: R√©ponse progressive token par token
- **Contexte enrichi**: Int√©gration des documents les plus pertinents
- **R√©f√©rences**: Sources avec scores de pertinence
- **Qualit√©**: R√©ponses bas√©es uniquement sur le contexte fourni

### üîå **Int√©gration MCP**
- **Serveur MCP natif**: Fonction `ask_rag_question()` expos√©e
- **Param√®tres configurables**: Nombre documents, activation reranking
- **Compatible**: Claude Desktop, VS Code, Cursor IDE
- **API structur√©e**: R√©ponses JSON avec sources et m√©tadonn√©es

## üöÄ Utilisation

### Interface Web
1. **Posez votre question** dans le chat
2. **Observez la recherche** en 2 √©tapes (vectorielle ‚Üí reranking)
3. **Lisez la r√©ponse** g√©n√©r√©e en streaming
4. **Consultez les sources** avec scores de pertinence

### Param√®tres Avanc√©s
- **Documents finaux**: Nombre de documents pour la g√©n√©ration (1-10)
- **Reranking**: Activer/d√©sactiver l'affinage Qwen3
- **Historique**: Conversations contextuelles

### Int√©gration MCP
Connectez votre client MCP pour un acc√®s programmatique :
```python
# Exemple d'utilisation MCP
result = mcp_client.call_tool(
    "ask_rag_question",
    question="Comment impl√©menter des r√©seaux de neurones complexes?",
    num_documents=3,
    use_reranking=True
)
```

## üéØ Cas d'Usage Parfaits

- **Documentation technique**: Recherche dans APIs, guides, tutoriels
- **Support client**: R√©ponses bas√©es sur une base de connaissances
- **Recherche acad√©mique**: Analyse de corpus documentaires
- **Assistance d√©veloppeur**: Aide contextuelle sur frameworks/librairies
- **Formation**: Syst√®me de questions-r√©ponses intelligent

## üìä Performance

- **Recherche**: ~50ms pour 10K+ documents
- **Reranking**: ~200ms pour 20 candidats  
- **G√©n√©ration**: ~2-4s avec streaming
- **M√©moire**: ~6-8GB optimis√© pour ZeroGPU

## üîí S√©curit√© & Confidentialit√©

- **ZeroGPU**: Traitement s√©curis√© sans stockage persistant
- **Donn√©es temporaires**: Pas de r√©tention des questions/r√©ponses
- **Mod√®les locaux**: Traitement dans l'environnement HF Spaces

## üìö Source des Donn√©es

Ce Space utilise des embeddings pr√©-calcul√©s depuis le dataset :
**[{space_info['repo_id']}](https://huggingface.co/datasets/{space_info['repo_id']})**

Commencez √† poser vos questions pour d√©couvrir la puissance du RAG avec Qwen3! üîç‚ú®
'''
    
    with open("README.md", "w", encoding="utf-8") as f:
        f.write(readme_content)
    
    print("‚úÖ README.md cr√©√©")

def create_requirements_txt():
    """Cr√©e le requirements.txt pour le Space"""
    print("üìù Cr√©ation de requirements.txt pour le Space...")
    
    requirements_content = '''gradio>=5.43.1
torch>=2.0.0
torchvision>=0.15.0
torchaudio>=2.0.0
transformers>=4.45.0
accelerate>=0.24.0
safetensors>=0.3.0
tokenizers>=0.15.0
sentencepiece>=0.1.97
huggingface_hub>=0.20.0
faiss-cpu>=1.7.0
numpy>=1.21.0
scipy>=1.9.0
python-dotenv>=1.0.0
sentence-transformers>=3.0.0
flash-attn>=2.0.0
spaces
'''
    
    with open("requirements.txt", "w") as f:
        f.write(requirements_content)
    
    print("‚úÖ requirements.txt cr√©√©")

def validate_files():
    """Valide que tous les fichiers requis existent"""
    print("üîç Validation des fichiers...")
    
    required_files = [
        "requirements.txt", 
        "README.md",
        "step03_chatbot.py",
        "step03_config.json"
    ]
    
    missing_files = []
    for file in required_files:
        if not os.path.exists(file):
            missing_files.append(file)
    
    if missing_files:
        print(f"‚ùå Fichiers manquants: {', '.join(missing_files)}")
        if "step03_config.json" in missing_files:
            print("üí° Lancez d'abord step02 pour g√©n√©rer la configuration")
        sys.exit(1)
    
    # V√©rifier les d√©corateurs ZeroGPU dans step03_chatbot.py
    with open("step03_chatbot.py", "r") as f:
        content = f.read()
        if "@spaces.GPU" not in content:
            print("‚ö†Ô∏è Attention: Pas de d√©corateurs @spaces.GPU trouv√©s dans step03_chatbot.py")
            print("Le Space fonctionnera mais sans optimisations GPU")
        else:
            print("‚úÖ D√©corateurs ZeroGPU d√©tect√©s")
    
    # V√©rifier le support MCP
    if "mcp_server=True" not in content and "/mcp/" not in content:
        print("‚ö†Ô∏è Attention: Support MCP non d√©tect√©")
    else:
        print("‚úÖ Support MCP d√©tect√©")
    
    print("‚úÖ Tous les fichiers requis pr√©sents")

def create_space(space_info, token):
    """Cr√©e ou met √† jour le Space HuggingFace"""
    print(f"üöÄ Cr√©ation/mise √† jour du Space: {space_info['name']}")
    
    from huggingface_hub import HfApi, login
    
    # Connexion √† HuggingFace
    login(token=token, add_to_git_credential=True)
    
    api = HfApi()
    
    try:
        # V√©rifier si le space existe
        space_info_api = api.space_info(repo_id=space_info['name'])
        print(f"‚úÖ Space {space_info['name']} existe d√©j√†, mise √† jour...")
        update_mode = True
    except Exception:
        print(f"üì¶ Cr√©ation du nouveau Space: {space_info['name']}")
        update_mode = False
        
        # Cr√©er le space
        try:
            api.create_repo(
                repo_id=space_info['name'],
                repo_type="space",
                space_sdk="gradio",
                space_hardware="zerogpu",
                private=space_info['private']
            )
            print("‚úÖ Space cr√©√© avec succ√®s")
        except Exception as e:
            print(f"‚ùå √âchec cr√©ation space: {e}")
            sys.exit(1)
    
    return update_mode

def deploy_files(space_info, token):
    """D√©ploie les fichiers vers le Space"""
    print("üì§ Upload des fichiers vers le Space...")
    
    from huggingface_hub import HfApi
    
    api = HfApi()
    
    # Fichiers √† uploader
    files_to_upload = [
        "requirements.txt",
        "README.md", 
        "step03_chatbot.py",
        "step03_config.json",
        "step03_utils.py"
    ]
    
    try:
        upload_results = []
        for file in files_to_upload:
            if os.path.exists(file):
                print(f"  üìÑ Upload {file}...")
                try:
                    api.upload_file(
                        path_or_fileobj=file,
                        path_in_repo=file,
                        repo_id=space_info['name'],
                        repo_type="space",
                        token=token
                    )
                    print(f"     ‚úÖ {file} upload√© avec succ√®s")
                    upload_results.append((file, True, None))
                except Exception as e:
                    print(f"     ‚ùå Erreur upload {file}: {e}")
                    upload_results.append((file, False, str(e)))
            else:
                print(f"  ‚ö†Ô∏è Fichier manquant ignor√©: {file}")
        
        # V√©rifier les r√©sultats
        failed_uploads = [r for r in upload_results if not r[1]]
        if failed_uploads:
            print(f"\n‚ö†Ô∏è {len(failed_uploads)} fichier(s) ont √©chou√©:")
            for filename, success, error in failed_uploads:
                print(f"  ‚ùå {filename}: {error}")
            print("\nüí° Le Space pourrait ne pas fonctionner correctement")
        else:
            print("‚úÖ Tous les fichiers upload√©s avec succ√®s")
        
    except Exception as e:
        print(f"‚ùå √âchec upload des fichiers: {e}")
        sys.exit(1)

def wait_for_space_build(space_info):
    """Attend que le Space se construise"""
    print("‚è≥ Le Space se construit... Cela peut prendre quelques minutes.")
    print(f"üåê Suivez la construction sur: https://huggingface.co/spaces/{space_info['name']}")
    print("üì± Le Space sera disponible une fois la construction termin√©e.")
    print("\n‚ö° Note: Le premier d√©marrage peut √™tre plus long (t√©l√©chargement mod√®les)")

def main():
    """Fonction principale de d√©ploiement"""
    parser = argparse.ArgumentParser(description="D√©ploiement LocalRAG vers HuggingFace Spaces")
    parser.add_argument("--space-name", help="Nom du Space (ex: username/space-name)")
    parser.add_argument("--private", action="store_true", help="Cr√©er un Space priv√©")
    args = parser.parse_args()
    
    print("üîç LocalRAG Step 04 - D√©ploiement HuggingFace Spaces")
    print("=" * 60)
    
    # Changer vers le r√©pertoire du script
    script_dir = Path(__file__).parent
    os.chdir(script_dir)
    
    # V√©rifier les d√©pendances
    check_dependencies()
    
    # Obtenir le token HuggingFace
    token = get_hf_token()
    
    # Obtenir les informations du Space
    if args.space_name:
        # Mode non-interactif avec args
        space_info = {
            'name': args.space_name,
            'title': 'üîç LocalRAG - RAG System with Qwen3',
            'description': 'Local RAG system with Qwen3 models and reranking',
            'repo_id': None,
            'private': args.private
        }
        
        # Charger repo_id depuis config
        config_file = Path("step03_config.json")
        if config_file.exists():
            with open(config_file, 'r') as f:
                config = json.load(f)
                space_info['repo_id'] = config.get('repo_id')
        
        if not space_info['repo_id']:
            print("‚ùå step03_config.json requis pour le repository embeddings")
            sys.exit(1)
    else:
        space_info = get_space_info()
    
    # Cr√©er les fichiers de configuration
    create_space_readme(space_info)
    # create_requirements_txt() # D√©sactiv√© - utiliser le requirements.txt existant
    
    # Valider les fichiers
    validate_files()
    
    # Cr√©er ou mettre √† jour le space
    update_mode = create_space(space_info, token)
    
    # D√©ployer les fichiers
    deploy_files(space_info, token)
    
    # Message de succ√®s
    print("\nüéâ D√©ploiement termin√© avec succ√®s!")
    print(f"üåê URL du Space: https://huggingface.co/spaces/{space_info['name']}")
    
    if not update_mode:
        wait_for_space_build(space_info)
    
    print(f"\nüì± Votre LocalRAG est maintenant en ligne:")
    print(f"   https://huggingface.co/spaces/{space_info['name']}")
    print(f"\nüöÄ Space d√©ploy√© avec ZeroGPU pour des performances optimales!")
    print(f"üîß Serveur MCP natif activ√© pour l'acc√®s programmatique!")
    print("\nüîç Bonne recherche dans vos documents! ‚ú®")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n‚ùå D√©ploiement annul√© par l'utilisateur")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå D√©ploiement √©chou√©: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)