#!/usr/bin/env python3
"""
Step 03 - Interface de chat RAG g√©n√©rique avec Gradio
Utilise les embeddings de Step 02 depuis Hugging Face Hub + Qwen3-4B-Instruct-2507 pour g√©n√©ration
"""

import os
import json
import numpy as np
import gradio as gr
from gradio import ChatMessage
from typing import List, Dict, Optional, Tuple
import time
import torch
import threading
import http.server
import socketserver
from pathlib import Path
from datetime import datetime

# ZeroGPU compatibility
try:
    import spaces
    ZEROGPU_AVAILABLE = True
    print("üöÄ ZeroGPU d√©tect√© - activation du support")
except ImportError:
    ZEROGPU_AVAILABLE = False
    # Fallback decorator for local usage
    class MockSpaces:
        @staticmethod
        def GPU(duration=None):
            def decorator(func):
                return func
            return decorator
    spaces = MockSpaces()

def _check_dependencies():
    """V√©rifie les d√©pendances n√©cessaires."""
    missing = []
    try:
        import torch
    except ImportError:
        missing.append("torch")
    
    try:
        import numpy as np
    except ImportError:
        missing.append("numpy")
        
    try:
        from safetensors.torch import load_file
    except ImportError:
        missing.append("safetensors")
        
    try:
        from huggingface_hub import hf_hub_download
    except ImportError:
        missing.append("huggingface-hub")
        
    try:
        import faiss
    except ImportError:
        missing.append("faiss-cpu")
        
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification
    except ImportError:
        missing.append("transformers")
        
    try:
        from sentence_transformers import SentenceTransformer
    except ImportError:
        missing.append("sentence-transformers")
    
    if missing:
        print(f"‚ùå D√©pendances manquantes: {', '.join(missing)}")
        print("üì¶ Installer avec: pip install " + " ".join(missing))
        return False
    return True


class Step03Config:
    """Gestionnaire de configuration Step 03 bas√© sur la sortie Step 02."""
    
    def __init__(self, config_file: str = "step03_config.json"):
        self.config_file = Path(config_file)
        self.config = self.load_config()
    
    def load_config(self) -> Dict:
        """Charge la configuration Step 03."""
        if not self.config_file.exists():
            raise FileNotFoundError(
                f"‚ùå Configuration Step 03 non trouv√©e: {self.config_file}\n"
                f"üí° Lancez d'abord: python step02_upload_embeddings.py"
            )
        
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            # V√©rification de la structure
            if not config.get("step02_completed"):
                raise ValueError("‚ùå Step 02 non compl√©t√© selon la configuration")
            
            required_keys = ["huggingface", "embeddings_info"]
            for key in required_keys:
                if key not in config:
                    raise ValueError(f"‚ùå Cl√© manquante dans configuration: {key}")
            
            return config
            
        except json.JSONDecodeError as e:
            raise ValueError(f"‚ùå Configuration Step 03 malform√©e: {e}")
    
    @property
    def repo_id(self) -> str:
        """Repository Hugging Face ID."""
        return self.config["huggingface"]["repo_id"]
    
    @property
    def dataset_name(self) -> str:
        """Nom du dataset."""
        return self.config["huggingface"]["dataset_name"]
    
    @property
    def embeddings_file(self) -> str:
        """Nom du fichier SafeTensors."""
        return self.config["huggingface"]["files"]["embeddings"]
    
    @property
    def metadata_file(self) -> str:
        """Nom du fichier m√©tadonn√©es."""
        return self.config["huggingface"]["files"]["metadata"]
    
    @property
    def total_vectors(self) -> int:
        """Nombre total de vecteurs."""
        return self.config["embeddings_info"]["total_vectors"]
    
    @property
    def vector_dimension(self) -> int:
        """Dimension des vecteurs."""
        return self.config["embeddings_info"]["vector_dimension"]
    
    @property
    def embedding_model(self) -> str:
        """Mod√®le d'embedding utilis√©."""
        return self.config["embeddings_info"]["embedding_model"]


class Qwen3Reranker:
    """
    Reranker utilisant Qwen3-Reranker-4B pour am√©liorer la pertinence des r√©sultats de recherche
    """
    
    def __init__(self, model_name: str = "Qwen/Qwen3-Reranker-4B", use_flash_attention: bool = True):
        """
        Initialise le reranker Qwen3
        
        Args:
            model_name: Nom du mod√®le HuggingFace √† charger
            use_flash_attention: Utiliser Flash Attention 2 si disponible (auto-d√©sactiv√© sur Mac)
        """
        self.model_name = model_name
        self.use_flash_attention = use_flash_attention
        
        # D√©tection de l'environnement
        self.is_mps = torch.backends.mps.is_available()
        self.is_cuda = torch.cuda.is_available()
        self.is_cpu = not self.is_mps and not self.is_cuda
        
        print(f"üîÑ Chargement du reranker {model_name}...")
        self._detect_platform()
        self._load_model()
    
    def _detect_platform(self):
        """D√©tecte la plateforme et ajuste les param√®tres"""
        if self.is_mps:
            print("  - Plateforme: Mac MPS d√©tect√©")
            self.use_flash_attention = False  # Flash Attention non compatible MPS
            self.batch_size = 1  # Traitement strictement individuel sur Mac
            self.memory_cleanup_freq = 3  # Nettoyage m√©moire fr√©quent
        elif self.is_cuda:
            print(f"  - Plateforme: CUDA d√©tect√© ({torch.cuda.get_device_name()})")
            self.batch_size = 1  # Garde traitement individuel pour stabilit√©
            self.memory_cleanup_freq = 10  # Nettoyage moins fr√©quent
        else:
            print("  - Plateforme: CPU")
            self.use_flash_attention = False
            self.batch_size = 1
            self.memory_cleanup_freq = 5
    
    def _load_model(self):
        """Charge le mod√®le et le tokenizer"""
        try:
            from transformers import AutoTokenizer, AutoModelForSequenceClassification
            
            # Chargement du tokenizer
            print("  - Chargement du tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            # Configuration du mod√®le selon la plateforme
            model_kwargs = self._get_model_config()
            
            # Chargement du mod√®le
            print("  - Chargement du mod√®le...")
            self.model = AutoModelForSequenceClassification.from_pretrained(
                self.model_name,
                **model_kwargs
            )
            
            # Configuration du device
            self._setup_device()
            
            print(f"‚úÖ Reranker charg√© sur {self.device}")
            print(f"  - Flash Attention: {'‚úÖ' if self.use_flash_attention else '‚ùå'}")
            print(f"  - Param√®tres: {self.get_parameter_count():.1f}B")
            
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement du reranker: {e}")
            print("üí° Le reranking sera d√©sactiv√©")
            self.model = None
            self.tokenizer = None
            self.device = None
    
    def _get_model_config(self) -> Dict:
        """Retourne la configuration du mod√®le selon la plateforme"""
        config = {}
        
        if self.is_mps:
            # Configuration pour Mac MPS
            config["torch_dtype"] = torch.float32  # MPS fonctionne mieux avec float32
            config["device_map"] = None  # device_map peut causer des probl√®mes avec MPS
        elif self.is_cuda:
            # Configuration pour CUDA
            config["torch_dtype"] = torch.float16
            if self.use_flash_attention:
                try:
                    config["attn_implementation"] = "flash_attention_2"
                    print("  - Flash Attention 2 activ√©e")
                except Exception:
                    print("  - Flash Attention 2 non disponible, utilisation standard")
                    self.use_flash_attention = False
            else:
                config["device_map"] = "auto"
        else:
            # Configuration pour CPU
            config["torch_dtype"] = torch.float32
            config["device_map"] = "cpu"
        
        return config
    
    def _setup_device(self):
        """Configure le device pour le mod√®le"""
        if self.is_mps:
            self.device = torch.device("mps")
            self.model = self.model.to(self.device)
        elif self.is_cuda:
            if hasattr(self.model, 'device'):
                self.device = next(self.model.parameters()).device
            else:
                self.device = torch.device("cuda")
                self.model = self.model.to(self.device)
        else:
            self.device = torch.device("cpu")
            self.model = self.model.to(self.device)
    
    def _format_pair(self, query: str, document: str, instruction: str = None) -> str:
        """
        Formate une paire query-document pour le reranker
        """
        if instruction:
            return f"Instruction: {instruction}\nQuery: {query}\nDocument: {document}"
        return f"Query: {query}\nDocument: {document}"
    
    def _get_default_instruction(self) -> str:
        """Retourne l'instruction par d√©faut pour la documentation technique"""
        return (
            "√âvaluez la pertinence de ce document technique "
            "par rapport √† la requ√™te en consid√©rant : terminologie technique, "
            "sp√©cifications, normes, proc√©dures de mise en ≈ìuvre."
        )
    
    def _process_single_document(self, query: str, document: str, instruction: str) -> float:
        """
        Traite un seul document et retourne son score de pertinence
        """
        # Formatage de la paire
        pair_text = self._format_pair(query, document, instruction)
        
        # Tokenisation (pas de probl√®me de padding avec un seul document)
        inputs = self.tokenizer(
            pair_text,
            truncation=True,
            max_length=512,
            return_tensors="pt",
            padding=False
        )
        
        # D√©placement vers le device
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # Inf√©rence
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits
            
            # Le mod√®le Qwen3-Reranker retourne des logits de forme [1, 2]
            # pour classification binaire : [non-pertinent, pertinent]
            probs = torch.nn.functional.softmax(logits, dim=1)
            score = probs[0, 1].cpu().item()  # Classe 1 = pertinent
            
            return float(score)
    
    def _cleanup_memory(self):
        """Nettoie la m√©moire selon la plateforme"""
        if self.is_mps:
            if hasattr(torch.mps, 'empty_cache'):
                torch.mps.empty_cache()
        elif self.is_cuda:
            torch.cuda.empty_cache()
        
        import gc
        gc.collect()
    
    @spaces.GPU(duration=60)  # ZeroGPU: alloue GPU pour 60s max pour reranking
    def rerank(self, query: str, documents: List[str], instruction: str = None) -> List[float]:
        """
        Reranke une liste de documents par rapport √† une requ√™te
        """
        if not documents:
            return []
        
        if self.model is None or self.tokenizer is None:
            print("  - Reranker non disponible, scores neutres retourn√©s")
            return [0.5] * len(documents)
        
        if instruction is None:
            instruction = self._get_default_instruction()
        
        print(f"  - Reranking de {len(documents)} documents (traitement individuel)")
        
        scores = []
        successful_count = 0
        
        for i, document in enumerate(documents):
            try:
                score = self._process_single_document(query, document, instruction)
                score = max(0.0, min(1.0, score))
                scores.append(score)
                successful_count += 1
                
                if (i + 1) % self.memory_cleanup_freq == 0:
                    self._cleanup_memory()
                
            except Exception as doc_error:
                print(f"    ‚ö†Ô∏è Erreur document {i+1}: {doc_error}")
                scores.append(0.5)  # Score neutre en cas d'erreur
        
        self._cleanup_memory()
        
        print(f"  ‚úÖ Reranking termin√©: {successful_count}/{len(documents)} documents trait√©s")
        
        if successful_count > 0:
            valid_scores = [s for s in scores if s != 0.5]
            if valid_scores:
                top_scores = sorted(valid_scores, reverse=True)[:3]
                print(f"  üìà Top 3 scores: {[f'{s:.3f}' for s in top_scores]}")
        
        return scores
    
    def get_parameter_count(self) -> float:
        """Retourne le nombre de param√®tres du mod√®le en milliards"""
        if self.model is None:
            return 0.0
        try:
            return sum(p.numel() for p in self.model.parameters()) / 1e9
        except:
            return 0.0
    
    def is_available(self) -> bool:
        """V√©rifie si le reranker est disponible et fonctionnel"""
        return self.model is not None and self.tokenizer is not None


class GenericRAGChatbot:
    """Chatbot RAG g√©n√©rique utilisant les embeddings de Step 02 et Qwen3-4B-Instruct pour la g√©n√©ration"""
    
    def __init__(self, 
                 generation_model: str = "Qwen/Qwen3-4B-Instruct-2507",
                 initial_k: int = 20,
                 final_k: int = 3,
                 use_flash_attention: bool = True,
                 use_reranker: bool = True):
        """
        Initialise le syst√®me RAG g√©n√©rique
        
        Args:
            generation_model: Mod√®le Qwen3 pour la g√©n√©ration
            initial_k: Nombre de candidats pour la recherche initiale
            final_k: Nombre de documents finaux apr√®s reranking
            use_flash_attention: Utiliser Flash Attention (d√©sactiv√© automatiquement sur Mac)
            use_reranker: Utiliser le reranking Qwen3
        """
        self.generation_model_name = generation_model
        self.initial_k = initial_k
        self.final_k = final_k
        self.use_flash_attention = use_flash_attention
        self.use_reranker = use_reranker
        
        # D√©tection de l'environnement (local + ZeroGPU)
        self.is_zerogpu = ZEROGPU_AVAILABLE and os.getenv("SPACE_ID") is not None
        self.is_mps = torch.backends.mps.is_available() and not self.is_zerogpu
        self.is_cuda = torch.cuda.is_available()
        
        # Configuration du device
        if self.is_mps:
            self.device = torch.device("mps")
        elif self.is_cuda:
            self.device = torch.device("cuda")
        else:
            self.device = torch.device("cpu")
        
        if self.is_zerogpu:
            print("üöÄ Environnement ZeroGPU d√©tect√© - optimisations cloud")
            self.use_flash_attention = True  # ZeroGPU supporte Flash Attention
        elif self.is_mps and use_flash_attention:
            print("üçé Mac avec MPS d√©tect√© - d√©sactivation automatique de Flash Attention")
            self.use_flash_attention = False
        
        # Chargement des composants
        self._load_step03_config()
        self._load_embeddings_from_hf()
        self._load_embedding_model()
        self._load_reranker()
        self._load_generation_model()

    def _load_step03_config(self):
        """Charge la configuration Step 03"""
        try:
            self.config = Step03Config()
            print(f"‚úÖ Configuration Step 03 charg√©e")
            print(f"  üì¶ Repository HF: {self.config.repo_id}")
            print(f"  üìä Embeddings: {self.config.total_vectors:,} vecteurs")
            print(f"  üìè Dimension: {self.config.vector_dimension}")
        except Exception as e:
            print(f"‚ùå Erreur de chargement de la configuration: {e}")
            raise

    def _load_embeddings_from_hf(self):
        """T√©l√©charge et charge les embeddings depuis Hugging Face Hub"""
        try:
            from huggingface_hub import hf_hub_download
            from safetensors.torch import load_file
            import numpy as np
            import faiss
            
            print(f"üîÑ T√©l√©chargement des embeddings depuis {self.config.repo_id}...")
            
            # T√©l√©charger les fichiers (sans token pour les repos publics)
            try:
                embeddings_file = hf_hub_download(
                    repo_id=self.config.repo_id,
                    filename=self.config.embeddings_file,
                    repo_type="dataset",
                    token=None  # Forcer l'acc√®s sans token pour les repos publics
                )
                
                metadata_file = hf_hub_download(
                    repo_id=self.config.repo_id,
                    filename=self.config.metadata_file,
                    repo_type="dataset",
                    token=None  # Forcer l'acc√®s sans token pour les repos publics
                )
            except Exception as auth_error:
                print(f"  ‚ö†Ô∏è Erreur d'authentification: {auth_error}")
                print("  üîë Essai avec token depuis les variables d'environnement...")
                
                # Essayer avec le token d'environnement
                import os
                hf_token = os.getenv('HF_TOKEN') or os.getenv('HUGGINGFACE_HUB_TOKEN')
                
                if hf_token:
                    print("  üîë Token trouv√©, nouvel essai...")
                    embeddings_file = hf_hub_download(
                        repo_id=self.config.repo_id,
                        filename=self.config.embeddings_file,
                        repo_type="dataset",
                        token=hf_token
                    )
                    
                    metadata_file = hf_hub_download(
                        repo_id=self.config.repo_id,
                        filename=self.config.metadata_file,
                        repo_type="dataset",
                        token=hf_token
                    )
                else:
                    print("  ‚ùå Aucun token trouv√© dans les variables d'environnement")
                    print("  üí° Solutions possibles:")
                    print("     1. V√©rifiez que le repository est bien public")
                    print("     2. Connectez-vous avec: huggingface-cli login")
                    print("     3. D√©finissez HF_TOKEN dans les variables d'environnement")
                    raise auth_error
            
            print("  üì• Chargement des embeddings SafeTensors...")
            tensors = load_file(embeddings_file)
            embeddings_tensor = tensors["embeddings"]
            embeddings_np = embeddings_tensor.numpy().astype(np.float32)
            
            print("  üìã Chargement des m√©tadonn√©es...")
            with open(metadata_file, 'r', encoding='utf-8') as f:
                self.metadata = json.load(f)
            
            # Cr√©er l'index FAISS (optimis√© pour Mac)
            print("  üîß Cr√©ation de l'index FAISS...")
            dimension = embeddings_np.shape[1]
            
            # Configuration d'index FAISS selon l'environnement
            if self.is_zerogpu:
                print("  üöÄ Index FAISS optimis√© pour ZeroGPU (IndexHNSWFlat)")
                # Index sophistiqu√© pour ZeroGPU avec GPU puissant
                self.faiss_index = faiss.IndexHNSWFlat(dimension, 32)
                self.faiss_index.hnsw.efConstruction = 200
                self.faiss_index.hnsw.efSearch = 50
            elif self.is_mps:
                print("  üçé Index FAISS optimis√© pour Mac (IndexFlatIP)")
                # Index simple mais efficace sur Mac
                self.faiss_index = faiss.IndexFlatIP(dimension)  # Inner Product (plus stable sur Mac)
            else:
                print("  üêß Index FAISS HNSW pour Linux/Windows")
                # Index plus sophistiqu√© pour autres plateformes
                self.faiss_index = faiss.IndexHNSWFlat(dimension, 32)
                self.faiss_index.hnsw.efConstruction = 200
                self.faiss_index.hnsw.efSearch = 50
            
            # Normaliser les embeddings pour IndexFlatIP (√©quivalent √† cosine similarity)
            if self.is_mps:
                # Normalisation L2 pour que IndexFlatIP = cosine similarity
                norms = np.linalg.norm(embeddings_np, axis=1, keepdims=True)
                embeddings_np = embeddings_np / (norms + 1e-8)  # √âviter division par 0
            
            print(f"  üìä Ajout de {embeddings_np.shape[0]:,} vecteurs √† l'index...")
            # Ajouter les vecteurs √† l'index
            self.faiss_index.add(embeddings_np)
            
            # R√©cup√©rer les mappings et m√©tadonn√©es de contenu
            self.ordered_ids = self.metadata.get('ordered_ids', [])
            self.id_to_idx = self.metadata.get('id_to_idx', {})
            self.content_metadata = self.metadata.get('content_metadata', {})
            
            
            print(f"‚úÖ Embeddings charg√©s: {embeddings_np.shape[0]:,} vecteurs de dimension {dimension}")
            
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement des embeddings: {e}")
            raise

    def _load_embedding_model(self):
        """Charge le mod√®le d'embeddings pour les requ√™tes"""
        print(f"üîÑ Chargement du mod√®le d'embeddings {self.config.embedding_model}...")
        
        try:
            from sentence_transformers import SentenceTransformer
            
            if self.use_flash_attention and self.is_cuda:
                print("  - Configuration avec Flash Attention 2 activ√©e (CUDA)")
                try:
                    self.embedding_model = SentenceTransformer(
                        self.config.embedding_model,
                        model_kwargs={
                            "attn_implementation": "flash_attention_2", 
                            "device_map": "auto"
                        },
                        tokenizer_kwargs={"padding_side": "left"}
                    )
                except Exception as flash_error:
                    print(f"  - Flash Attention √©chou√©: {flash_error}")
                    print("  - Fallback vers configuration standard")
                    self.embedding_model = SentenceTransformer(self.config.embedding_model)
                    self.use_flash_attention = False
            else:
                print("  - Configuration standard (MPS/CPU ou Flash Attention d√©sactiv√©)")
                model_kwargs = {}
                
                if self.is_mps:
                    model_kwargs = {"torch_dtype": torch.float32}
                
                if model_kwargs:
                    self.embedding_model = SentenceTransformer(
                        self.config.embedding_model,
                        model_kwargs=model_kwargs,
                        tokenizer_kwargs={"padding_side": "left"}
                    )
                else:
                    self.embedding_model = SentenceTransformer(self.config.embedding_model)
                
            print(f"‚úÖ Mod√®le d'embeddings {self.config.embedding_model} charg√© avec succ√®s")
            
        except Exception as e:
            print(f"‚ùå Erreur avec {self.config.embedding_model}: {e}")
            print("üîÑ Fallback vers le mod√®le multilingual MiniLM...")
            self.embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
            self.use_flash_attention = False

    def _load_reranker(self):
        """Charge le reranker Qwen3-Reranker-4B"""
        if self.use_reranker:
            try:
                effective_flash_attention = self.use_flash_attention and not self.is_mps
                self.reranker = Qwen3Reranker(use_flash_attention=effective_flash_attention)
            except Exception as e:
                print(f"‚ùå Erreur lors du chargement du reranker: {e}")
                print("üîÑ D√©sactivation du reranking")
                self.use_reranker = False
                self.reranker = None
        else:
            self.reranker = None
            print("‚ö†Ô∏è Reranking d√©sactiv√© par configuration")

    def _load_generation_model(self):
        """Charge le mod√®le de g√©n√©ration Qwen3-4B-Instruct"""
        print(f"üîÑ Chargement du mod√®le de g√©n√©ration {self.generation_model_name}...")
        
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM
            
            # Chargement du tokenizer
            print("  - Chargement du tokenizer...")
            self.generation_tokenizer = AutoTokenizer.from_pretrained(self.generation_model_name)
            
            # Configuration du mod√®le selon la plateforme
            model_kwargs = self._get_generation_model_config()
            
            # Chargement du mod√®le
            print("  - Chargement du mod√®le...")
            self.generation_model = AutoModelForCausalLM.from_pretrained(
                self.generation_model_name,
                **model_kwargs
            )
            
            # Configuration du device
            self._setup_generation_device()
            
            print(f"‚úÖ Mod√®le de g√©n√©ration charg√© sur {self.generation_device}")
            print(f"  - Param√®tres: {self._get_generation_parameter_count():.1f}B")
            
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement du mod√®le de g√©n√©ration: {e}")
            print("üí° La g√©n√©ration sera d√©sactiv√©e")
            self.generation_model = None
            self.generation_tokenizer = None
            self.generation_device = None

    def _get_generation_model_config(self) -> Dict:
        """Retourne la configuration du mod√®le de g√©n√©ration selon la plateforme"""
        config = {}
        
        if self.is_mps:
            config["torch_dtype"] = torch.float32
            config["device_map"] = None
        elif self.is_cuda:
            config["torch_dtype"] = torch.float16
            if self.use_flash_attention:
                try:
                    config["attn_implementation"] = "flash_attention_2"
                    print("  - Flash Attention 2 activ√©e pour g√©n√©ration")
                except Exception:
                    print("  - Flash Attention 2 non disponible pour g√©n√©ration")
            config["device_map"] = "auto"
        else:
            config["torch_dtype"] = torch.float32
            config["device_map"] = "cpu"
        
        return config

    def _setup_generation_device(self):
        """Configure le device pour le mod√®le de g√©n√©ration"""
        if self.is_mps:
            self.generation_device = torch.device("mps")
            self.generation_model = self.generation_model.to(self.generation_device)
        elif self.is_cuda:
            if hasattr(self.generation_model, 'device'):
                self.generation_device = next(self.generation_model.parameters()).device
            else:
                self.generation_device = torch.device("cuda")
                self.generation_model = self.generation_model.to(self.generation_device)
        else:
            self.generation_device = torch.device("cpu")
            self.generation_model = self.generation_model.to(self.generation_device)

    def _get_generation_parameter_count(self) -> float:
        """Retourne le nombre de param√®tres du mod√®le de g√©n√©ration en milliards"""
        if self.generation_model is None:
            return 0.0
        try:
            return sum(p.numel() for p in self.generation_model.parameters()) / 1e9
        except:
            return 0.0

    def search_documents(self, query: str, final_k: int = None, use_reranking: bool = None) -> List[Dict]:
        """
        Recherche avanc√©e avec reranking en deux √©tapes
        """
        k = final_k if final_k is not None else self.final_k
        initial_k = max(self.initial_k, k * 3)
        should_rerank = use_reranking if use_reranking is not None else self.use_reranker
        
        print(f"üîç Recherche en deux √©tapes: {initial_k} candidats ‚Üí reranking ‚Üí {k} finaux")
        
        # √âtape 1: Recherche par embedding avec FAISS
        if hasattr(self.embedding_model, 'prompts') and 'query' in self.embedding_model.prompts:
            query_embedding = self.embedding_model.encode([query], prompt_name="query")[0]
        else:
            query_embedding = self.embedding_model.encode([query])[0]
        
        # Recherche dans l'index FAISS
        query_vector = query_embedding.reshape(1, -1).astype('float32')
        
        # Normaliser la requ√™te sur Mac pour IndexFlatIP (consistency avec les embeddings)
        if self.is_mps:
            norm = np.linalg.norm(query_vector)
            if norm > 0:
                query_vector = query_vector / norm
        
        distances, indices = self.faiss_index.search(query_vector, initial_k)
        
        if len(indices[0]) == 0:
            print("‚ùå Aucun document trouv√©")
            return []
        
        print(f"üìã {len(indices[0])} candidats r√©cup√©r√©s")
        
        # Conversion en format interm√©diaire
        initial_results = []
        for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
            if idx < len(self.ordered_ids):
                doc_id = self.ordered_ids[idx]
                doc_metadata = self.content_metadata.get(doc_id, {})
                
                # Ajustement des scores selon le type d'index
                if self.is_mps:
                    # Sur Mac avec IndexFlatIP : distance = inner product (plus haut = plus similaire)
                    embedding_score = float(distance)  # Inner product normalis√© = cosine similarity
                    embedding_distance = 1.0 - embedding_score  # Conversion en distance pour compatibilit√©
                else:
                    # Sur autres plateformes avec IndexHNSWFlat : distance euclidienne
                    embedding_distance = float(distance)
                    embedding_score = 1 - embedding_distance
                
                doc = {
                    'content': doc_metadata.get('chunk_content', 'Contenu non disponible'),
                    'metadata': doc_metadata,
                    'embedding_distance': embedding_distance,
                    'embedding_score': embedding_score,
                    'source': doc_metadata.get('source_file', 'Inconnu'),
                    'title': doc_metadata.get('title', 'Sans titre'),
                    'heading': doc_metadata.get('heading', ''),
                    'initial_rank': i + 1
                }
                initial_results.append(doc)
        
        # √âtape 2: Reranking si disponible
        if should_rerank and self.reranker and self.reranker.model is not None:
            print("üéØ Application du reranking Qwen3...")
            
            documents = [doc['content'] for doc in initial_results]
            
            
            rerank_scores = self.reranker.rerank(query, documents)
            
            # Ajout des scores de reranking
            for doc, rerank_score in zip(initial_results, rerank_scores):
                doc['rerank_score'] = float(rerank_score)
            
            # Tri par score de reranking
            initial_results.sort(key=lambda x: x['rerank_score'], reverse=True)
            
            # Mise √† jour des positions finales
            for i, doc in enumerate(initial_results):
                doc['final_rank'] = i + 1
            
            print(f"‚úÖ Reranking appliqu√©, top 5 scores: {[f'{doc['rerank_score']:.3f}' for doc in initial_results[:5]]}")
        else:
            print("‚ö†Ô∏è Reranking d√©sactiv√©, utilisation des scores d'embedding uniquement")
            for doc in initial_results:
                doc['rerank_score'] = doc['embedding_score']
                doc['final_rank'] = doc['initial_rank']
        
        # Retour des top-k r√©sultats finaux
        final_results = initial_results[:k]
        print(f"üìä {len(final_results)} documents finaux s√©lectionn√©s")
        
        return final_results

    @spaces.GPU(duration=120)  # ZeroGPU: alloue GPU pour 120s max pour g√©n√©ration
    def generate_response_stream(self, query: str, context: str, history: List = None):
        """
        G√©n√®re une r√©ponse stream√©e bas√©e sur le contexte et l'historique
        """
        if self.generation_model is None or self.generation_tokenizer is None:
            yield "‚ùå Mod√®le de g√©n√©ration non disponible"
            return
        
        # Construction du prompt syst√®me
        system_prompt = """Tu es un assistant expert qui r√©pond aux questions en te basant uniquement sur les documents fournis dans le contexte.

Instructions importantes:
- R√©ponds en fran√ßais de mani√®re claire et pr√©cise
- Base-toi uniquement sur les informations du contexte fourni
- Si l'information n'est pas dans le contexte, dis-le clairement
- Utilise un ton professionnel adapt√© au domaine
- Structure ta r√©ponse avec des paragraphes clairs"""
        
        # Construire le prompt complet
        messages = [{"role": "system", "content": system_prompt}]
        
        # Ajouter l'historique si fourni
        if history:
            for msg in history:
                if hasattr(msg, 'role') and hasattr(msg, 'content'):
                    messages.append({"role": msg.role, "content": msg.content})
        
        # Ajouter le contexte et la question
        user_message = f"Contexte:\n{context}\n\nQuestion: {query}"
        messages.append({"role": "user", "content": user_message})
        
        try:
            # Tokenisation
            inputs = self.generation_tokenizer.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_tensors="pt"
            ).to(self.device)
            
            # G√©n√©ration stream√©e
            from transformers import TextIteratorStreamer
            import threading
            
            streamer = TextIteratorStreamer(
                self.generation_tokenizer,
                timeout=10.0,
                skip_prompt=True,
                skip_special_tokens=True
            )
            
            generation_kwargs = {
                "input_ids": inputs,
                "streamer": streamer,
                "max_new_tokens": 1024,
                "temperature": 0.7,
                "do_sample": True,
                "pad_token_id": self.generation_tokenizer.eos_token_id,
                "eos_token_id": self.generation_tokenizer.eos_token_id,
            }
            
            # Lancer la g√©n√©ration dans un thread s√©par√©
            thread = threading.Thread(target=self.generation_model.generate, kwargs=generation_kwargs)
            thread.start()
            
            # Streamer les tokens
            for new_token in streamer:
                yield new_token
            
            thread.join()
            
        except Exception as e:
            yield f"‚ùå Erreur lors de la g√©n√©ration: {str(e)}"

    @spaces.GPU(duration=120)  # ZeroGPU: alloue GPU pour 120s max pour g√©n√©ration  
    def generate_response(self, query: str, context: str, history: List = None) -> str:
        """
        G√©n√®re une r√©ponse bas√©e sur le contexte et l'historique
        """
        if self.generation_model is None or self.generation_tokenizer is None:
            return "‚ùå Mod√®le de g√©n√©ration non disponible"
        
        # Construction du prompt syst√®me
        system_prompt = """Tu es un assistant expert qui r√©pond aux questions en te basant uniquement sur les documents fournis dans le contexte.

Instructions importantes:
- R√©ponds en fran√ßais de mani√®re claire et pr√©cise
- Base-toi uniquement sur les informations du contexte fourni
- Si l'information n'est pas dans le contexte, dis-le clairement
- Utilise un ton professionnel adapt√© au domaine
- Structure ta r√©ponse avec des paragraphes clairs"""
        
        # Construire le prompt complet
        messages = [{"role": "system", "content": system_prompt}]
        
        # Ajouter l'historique si fourni
        if history:
            for msg in history:
                if hasattr(msg, 'role') and hasattr(msg, 'content'):
                    if msg.role in ["user", "assistant"] and not getattr(msg, 'metadata', None):
                        messages.append({"role": msg.role, "content": msg.content})
        
        # Ajouter la question courante avec le contexte
        user_prompt = f"""Contexte documentaire:
{context}

Question: {query}

R√©ponds √† cette question en te basant sur le contexte fourni."""
        
        messages.append({"role": "user", "content": user_prompt})
        
        # Formatage pour le mod√®le
        try:
            # Appliquer le template de chat du mod√®le
            formatted_prompt = self.generation_tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # Tokenisation
            inputs = self.generation_tokenizer(
                formatted_prompt,
                return_tensors="pt",
                truncation=True,
                max_length=4096
            )
            
            # D√©placement vers le device
            inputs = {k: v.to(self.generation_device) for k, v in inputs.items()}
            
            # G√©n√©ration
            with torch.no_grad():
                outputs = self.generation_model.generate(
                    **inputs,
                    max_new_tokens=1024,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.generation_tokenizer.eos_token_id,
                    eos_token_id=self.generation_tokenizer.eos_token_id,
                )
            
            # D√©codage de la r√©ponse
            full_response = self.generation_tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Extraire seulement la nouvelle g√©n√©ration
            response = full_response[len(formatted_prompt):].strip()
            
            return response
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration: {e}")
            return f"‚ùå Erreur lors de la g√©n√©ration de la r√©ponse: {str(e)}"

    def stream_response_with_tools(self, query: str, history, top_k: int = None, use_reranking: bool = None):
        """
        G√©n√®re une r√©ponse stream√©e avec affichage visuel des tools et reranking Qwen3
        """
        # 1. S'assurer que l'historique est une liste
        if not history:
            history = []
        
        # 2. Ajouter le message utilisateur seulement s'il n'est pas d√©j√† pr√©sent
        if not history or history[-1].role != "user" or history[-1].content != query:
            history.append(ChatMessage(role="user", content=query))
            yield history
            time.sleep(0.1)
        
        # 3. Recherche des documents avec tool visuel
        should_rerank = use_reranking if use_reranking is not None else self.use_reranker
        search_method = "avec reranking Qwen3" if should_rerank else "par embedding seulement"
        
        history.append(ChatMessage(
            role="assistant",
            content=f"Je recherche les documents les plus pertinents dans la base de donn√©es ({search_method})...",
            metadata={"title": "üîç Recherche s√©mantique avanc√©e"}
        ))
        yield history
        
        # Recherche des documents pertinents
        relevant_docs = self.search_documents(query, top_k, use_reranking)
        
        time.sleep(0.2)
        
        if not relevant_docs:
            history.append(ChatMessage(
                role="assistant",
                content="Aucun document pertinent trouv√© dans la base de donn√©es."
            ))
            yield history
            return
        
        # 4. Affichage des documents trouv√©s avec scores d√©taill√©s
        docs_summary = f"Trouv√© {len(relevant_docs)} documents pertinents"
        if should_rerank:
            docs_summary += f"\n\nüìä **Reranking Qwen3 appliqu√©:**"
            for i, doc in enumerate(relevant_docs):
                embedding_score = doc.get('embedding_score', 0)
                rerank_score = doc.get('rerank_score', 0)
                rank_change = doc.get('initial_rank', i+1) - doc.get('final_rank', i+1)
                rank_indicator = f" (#{doc.get('initial_rank', i+1)}‚Üí#{doc.get('final_rank', i+1)})" if rank_change != 0 else ""
                docs_summary += f"\n‚Ä¢ **{doc['title']}**{rank_indicator}"
                docs_summary += f"\n  ‚îî Embedding: {embedding_score:.3f} | Reranking: {rerank_score:.3f}"
        else:
            for i, doc in enumerate(relevant_docs):
                embedding_score = doc.get('embedding_score', doc.get('distance', 0))
                docs_summary += f"\n‚Ä¢ **{doc['title']}** - Score: {embedding_score:.3f}"
        
        history.append(ChatMessage(
            role="assistant",
            content=docs_summary,
            metadata={"title": f"üìö Documents s√©lectionn√©s ({len(relevant_docs)} total)"}
        ))
        yield history
        
        time.sleep(0.2)
        
        # 5. Construction du contexte
        context_parts = []
        sources_with_scores = []

        for i, doc in enumerate(relevant_docs):
            context_parts.append(f"[Document {i+1}] {doc['title']} - {doc['heading']}\n{doc['content']}")
            sources_with_scores.append({
                'title': doc['title'],
                'source': doc['source'],
                'embedding_score': doc.get('embedding_score', 1 - doc.get('distance', 0)),
                'rerank_score': doc.get('rerank_score'),
                'final_rank': doc.get('final_rank', i+1)
            })

        context = "\n\n".join(context_parts)
        
        # 6. G√©n√©ration de la r√©ponse avec Qwen3-4B
        history.append(ChatMessage(
            role="assistant",
            content="G√©n√©ration de la r√©ponse bas√©e sur les documents s√©lectionn√©s...",
            metadata={"title": "ü§ñ G√©n√©ration avec Qwen3-4B"}
        ))
        yield history
        
        time.sleep(0.2)
        
        # G√©n√©ration stream√©e de la r√©ponse
        history.append(ChatMessage(
            role="assistant", 
            content="",  # Commencer avec un contenu vide
            metadata={"title": "ü§ñ R√©ponse g√©n√©r√©e"}
        ))
        
        # Streamer la r√©ponse token par token
        current_response = ""
        for token in self.generate_response_stream(query, context, history[:-1]):  # Exclure le dernier message vide
            current_response += token
            # Mettre √† jour le dernier message avec la r√©ponse en cours
            history[-1] = ChatMessage(
                role="assistant",
                content=current_response,
                metadata={"title": "ü§ñ R√©ponse g√©n√©r√©e"}
            )
            yield history
            time.sleep(0.01)  # Petit d√©lai pour un streaming fluide
        
        time.sleep(0.2)
        
        # 7. Ajout des sources consult√©es avec scores d√©taill√©s
        sources_text = []
        for i, source_info in enumerate(sources_with_scores):
            embedding_score = source_info['embedding_score']
            rerank_score = source_info.get('rerank_score')
            source_file = source_info['source']
            
            if rerank_score is not None:
                score_display = f"Embedding: {embedding_score:.3f} | **Reranking: {rerank_score:.3f}**"
            else:
                score_display = f"Score: {embedding_score:.3f}"
            
            sources_text.append(f"‚Ä¢ **[{i+1}]** {source_info['title']} ({source_file})\n  ‚îî {score_display}")
        
        sources_display = "\n".join(sources_text)
        
        # Titre adaptatif selon la m√©thode utilis√©e
        sources_title = f"üìö Sources avec reranking Qwen3 ({len(relevant_docs)} documents)" if should_rerank else f"üìö Sources par embedding ({len(relevant_docs)} documents)"
        
        history.append(ChatMessage(
            role="assistant",
            content=sources_display,
            metadata={"title": sources_title}
        ))
        yield history


def _create_rag_system():
    """Cr√©√© et configure le syst√®me RAG avec param√®tres optimaux"""
    
    # D√©tection automatique d'environnement
    is_zerogpu = ZEROGPU_AVAILABLE and os.getenv("SPACE_ID") is not None
    is_mac = torch.backends.mps.is_available() and not is_zerogpu
    is_cuda = torch.cuda.is_available()
    
    if is_zerogpu:
        print("üöÄ ZeroGPU d√©tect√© - optimisations cloud appliqu√©es")
    elif is_mac:
        print("üçé Mac avec MPS d√©tect√© - optimisations automatiques appliqu√©es")
    elif is_cuda:
        print("üêß CUDA d√©tect√© - optimisations GPU appliqu√©es")
    else:
        print("üíª CPU d√©tect√© - optimisations processeur appliqu√©es")
    
    # Param√®tres par d√©faut optimis√©s selon l'environnement
    if is_zerogpu:
        default_config = {
            'use_flash_attention': True,   # ZeroGPU supporte Flash Attention
            'use_reranker': True,          # GPU puissant, reranking activ√©
            'initial_k': 30,               # Plus de candidats avec GPU puissant
            'final_k': 5                   # Plus de documents finaux
        }
    elif is_mac:
        default_config = {
            'use_flash_attention': False,  # MPS ne supporte pas Flash Attention
            'use_reranker': True,          # Reranking OK sur Mac
            'initial_k': 20,               # Valeurs mod√©r√©es
            'final_k': 3
        }
    else:
        default_config = {
            'use_flash_attention': is_cuda,  # Flash Attention seulement sur CUDA
            'use_reranker': True,            # Reranking par d√©faut
            'initial_k': 20,                 # Candidats pour la premi√®re √©tape
            'final_k': 3                     # Documents finaux par d√©faut
        }
    
    print("üöÄ Initialisation du chatbot RAG g√©n√©rique...")
    return GenericRAGChatbot(**default_config)


def _clear_message():
    """Fonction utilitaire interne pour effacer le message d'entr√©e."""
    return ""

def _clear_chat():
    """Fonction utilitaire interne pour effacer l'historique de chat."""
    return []

def _ensure_chatmessages(history):
    """Convertit une liste en objets ChatMessage si besoin."""
    result = []
    for m in history or []:
        if isinstance(m, ChatMessage):
            result.append(m)
        elif isinstance(m, dict):
            result.append(ChatMessage(
                role=m.get("role", ""),
                content=m.get("content", ""),
                metadata=m.get("metadata", None)
            ))
        elif isinstance(m, (list, tuple)) and len(m) >= 2:
            result.append(ChatMessage(role=m[0], content=m[1]))
    return result


def chat_with_generic_rag(message, history, top_k, use_reranking):
    """
    Interface entre Gradio et le syst√®me RAG g√©n√©rique avec contr√¥les avanc√©s.
    
    Cette fonction g√®re l'interface de chat interactive avec streaming en temps r√©el
    et affichage des √©tapes de traitement (recherche, reranking, g√©n√©ration).
    
    Args:
        message (str): Le message ou question de l'utilisateur √† traiter
        history (list): L'historique de la conversation sous forme de liste de messages
        top_k (int): Nombre de documents finaux √† utiliser pour la g√©n√©ration de r√©ponse
        use_reranking (bool): Activation du reranking Qwen3 pour am√©liorer la s√©lection
        
    Yields:
        list: Historique mis √† jour avec les nouveaux messages et √©tapes de traitement
    """
    history = _ensure_chatmessages(history)
    response_generator = rag_system.stream_response_with_tools(message, history, top_k, use_reranking)
    for updated_history in response_generator:
        yield updated_history


def ask_rag_question(question: str = "Qu'est-ce que Swift MLX?", num_documents: int = 3, use_reranking: bool = True) -> str:
    """
    Pose une question au syst√®me RAG LocalRAG et retourne la r√©ponse avec les documents sources.
    
    Cette fonction utilise un syst√®me de recherche s√©mantique avanc√© avec des mod√®les Qwen3
    pour interroger une base de connaissances et g√©n√©rer des r√©ponses contextualis√©es.

    Args:
        question (str): La question √† poser au syst√®me RAG en langage naturel
        num_documents (int): Nombre de documents √† utiliser pour g√©n√©rer la r√©ponse (entre 1 et 10)
        use_reranking (bool): Utiliser le reranking Qwen3-Reranker-4B pour am√©liorer la s√©lection des documents

    Returns:
        str: R√©ponse g√©n√©r√©e incluant la r√©ponse contextuelle et les sources avec leurs scores de pertinence
    """
    global rag_system
    
    try:
        # Validation des param√®tres
        num_documents = max(1, min(10, int(num_documents)))
        
        print(f"üîç Question MCP: {question}")
        print(f"üìä Param√®tres: {num_documents} documents, reranking: {use_reranking}")
        
        # Recherche des documents pertinents
        relevant_docs = rag_system.search_documents(question, num_documents, use_reranking)
        
        if not relevant_docs:
            return "‚ùå Aucun document pertinent trouv√© dans la base de donn√©es pour r√©pondre √† cette question."
        
        # Construction du contexte pour la g√©n√©ration
        context_parts = []
        for i, doc in enumerate(relevant_docs):
            context_parts.append(f"[Document {i+1}] {doc['title']} - {doc['heading']}\n{doc['content']}")
        
        context = "\n\n".join(context_parts)
        
        # G√©n√©ration de la r√©ponse
        response = rag_system.generate_response(question, context, None)
        
        # Formatage de la r√©ponse avec les sources
        sources_info = []
        search_method = "avec reranking Qwen3" if use_reranking else "par embedding seulement"
        
        sources_info.append(f"\n\nüìö **Documents sources utilis√©s ({search_method}):**\n")
        
        for i, doc in enumerate(relevant_docs):
            embedding_score = doc.get('embedding_score', 0)
            rerank_score = doc.get('rerank_score')
            initial_rank = doc.get('initial_rank', i+1)
            final_rank = doc.get('final_rank', i+1)
            
            # Formatage des scores
            if rerank_score is not None and use_reranking:
                score_display = f"Embedding: {embedding_score:.3f} | **Reranking: {rerank_score:.3f}**"
                if initial_rank != final_rank:
                    rank_change = f" (#{initial_rank}‚Üí#{final_rank})"
                else:
                    rank_change = ""
            else:
                score_display = f"Score: {embedding_score:.3f}"
                rank_change = ""
                
            sources_info.append(f"‚Ä¢ **[{i+1}]** {doc['title']}{rank_change}")
            sources_info.append(f"  ‚îî {score_display}")
            sources_info.append(f"  ‚îî Source: {doc['source']}")
        
        # Assemblage de la r√©ponse finale
        final_response = response + "\n".join(sources_info)
        
        print(f"‚úÖ R√©ponse MCP g√©n√©r√©e ({len(relevant_docs)} documents utilis√©s)")
        return final_response
        
    except Exception as e:
        error_msg = f"‚ùå Erreur lors du traitement de la question: {str(e)}"
        print(error_msg)
        return error_msg


def main():
    """Point d'entr√©e principal."""
    print("üöÄ LocalRAG Step 03 - Interface de chat g√©n√©rique")
    print("=" * 50)
    
    # V√©rification des d√©pendances
    if not _check_dependencies():
        return 1
    
    # Initialisation du syst√®me RAG
    global rag_system
    try:
        rag_system = _create_rag_system()
    except Exception as e:
        print(f"‚ùå Erreur d'initialisation: {e}")
        return 1
    
    # Configuration de l'interface Gradio avec th√®me Glass
    with gr.Blocks(
        title="ü§ñ LocalRAG Chat G√©n√©rique",
        theme=gr.themes.Glass(),
    ) as demo:
        
        # En-t√™te simplifi√© avec composants Gradio natifs
        with gr.Row():
            with gr.Column():
                gr.Markdown("# ü§ñ Assistant RAG G√©n√©rique LocalRAG")
                
                # Affichage de l'environnement d'ex√©cution
                env_info = ""
                if ZEROGPU_AVAILABLE and os.getenv("SPACE_ID"):
                    env_info = "üöÄ **Powered by ZeroGPU** - GPU gratuit Hugging Face"
                elif torch.backends.mps.is_available():
                    env_info = "üçé **Apple Silicon optimis√©** - MPS accelerated"
                elif torch.cuda.is_available():
                    env_info = f"üêß **CUDA accelerated** - {torch.cuda.get_device_name()}"
                else:
                    env_info = "üíª **CPU optimis√©** - Traitement local"
                
                gr.Markdown(f"**Syst√®me RAG complet avec mod√®les Qwen3 de derni√®re g√©n√©ration**")
                gr.Markdown(env_info)
                gr.Markdown(f"üß† {rag_system.config.embedding_model.split('/')[-1]} ‚Ä¢ üéØ Qwen3-Reranker-4B ‚Ä¢ üí¨ Qwen3-4B ‚Ä¢ ‚ö° Recherche en 2 √©tapes")
                gr.Markdown(f"üì¶ Repository: `{rag_system.config.repo_id}` | üìä Vecteurs: **{rag_system.config.total_vectors:,}**")
        
        # Interface de chat
        chatbot = gr.Chatbot(
            height=500,
            show_label=False,
            container=True,
            show_copy_button=True,
            autoscroll=True,
            avatar_images=(None, "ü§ñ"),
            type="messages"
        )
        
        # Zone de saisie
        with gr.Row():
            msg = gr.Textbox(
                placeholder="Posez votre question...",
                show_label=False,
                container=False,
                scale=4
            )
            send_btn = gr.Button("üì§ Envoyer", variant="primary", scale=1)
        
        # Panneau de contr√¥le avanc√© simplifi√©
        with gr.Accordion("üéõÔ∏è Contr√¥les avanc√©s", open=True):
            with gr.Row():
                top_k_slider = gr.Slider(
                    minimum=1,
                    maximum=10,
                    value=3,
                    step=1,
                    label="üìä Nombre de documents finaux",
                    info="Documents qui seront utilis√©s pour g√©n√©rer la r√©ponse"
                )
                
                reranking_checkbox = gr.Checkbox(
                    value=True,
                    label="üéØ Activer le reranking Qwen3",
                    info="Am√©liore la pertinence avec un mod√®le de reranking sp√©cialis√©"
                )
        
        # Bouton pour effacer
        clear_btn = gr.Button("üóëÔ∏è Effacer la conversation", variant="secondary", size="lg")
        
        # Informations en pied de page avec Accordion pour √©conomiser l'espace
        with gr.Accordion("‚ÑπÔ∏è Informations sur l'architecture", open=False):
            env_docs = ""
            if ZEROGPU_AVAILABLE and os.getenv("SPACE_ID"):
                env_docs = """
            ### üöÄ Optimisations ZeroGPU
            
            - **Allocation dynamique :** GPU allou√© automatiquement pour le reranking et la g√©n√©ration
            - **NVIDIA H200 :** 70GB VRAM disponible pour les calculs intensifs
            - **D√©corateurs intelligents :** `@spaces.GPU()` pour optimiser l'usage GPU
            - **Cache optimis√© :** Stockage temporaire en `/tmp` pour performances maximales
            """
            elif torch.backends.mps.is_available():
                env_docs = """
            ### üçé Optimisations Apple Silicon
            
            - **Metal Performance Shaders :** Acc√©l√©ration native Apple
            - **Index FAISS adapt√© :** IndexFlatIP pour √©viter les segfaults
            - **M√©moire unifi√©e :** Partage efficace CPU/GPU
            - **Float32 :** Pr√©cision optimis√©e pour MPS
            """
            else:
                env_docs = """
            ### ‚ö° Optimisations locales
            
            - **Multi-plateforme :** Support CPU, CUDA, MPS selon disponibilit√©
            - **Flash Attention :** Activ√© automatiquement sur CUDA
            - **Gestion m√©moire :** Cleanup automatique pour stabilit√©
            """
            
            gr.Markdown(f"""
            ### üöÄ Architecture LocalRAG Step 03
            
            - **üì• Step 02 :** Embeddings charg√©s depuis Hugging Face Hub au format SafeTensors
            - **üîç Recherche :** Index FAISS reconstruct√© pour recherche vectorielle haute performance
            - **üéØ Reranking :** Qwen3-Reranker-4B pour affiner la s√©lection des documents
            - **üí¨ G√©n√©ration :** Qwen3-4B-Instruct-2507 pour des r√©ponses contextuelles optimis√©es
            {env_docs}
            ### üìä Lecture des scores
            
            - **Score Embedding :** Similarit√© vectorielle initiale (0.0-1.0, plus haut = plus pertinent)
            - **Score Reranking :** Score de pertinence final apr√®s analyse contextuelle
            - **Changement de rang :** Evolution de la position du document apr√®s reranking
            """)

        # Gestionnaire de likes
        def like_response(evt: gr.LikeData):
            print(f"R√©action utilisateur: {'üëç' if evt.liked else 'üëé'} sur le message #{evt.index}")
            print(f"Contenu: {evt.value[:100]}...")
        
        chatbot.like(like_response)
        
        # Envoi par touche Entr√©e
        msg.submit(
            chat_with_generic_rag,
            [msg, chatbot, top_k_slider, reranking_checkbox],
            chatbot
        ).then(
            _clear_message,
            outputs=msg
        )
        
        # Envoi par bouton
        send_btn.click(
            chat_with_generic_rag,
            [msg, chatbot, top_k_slider, reranking_checkbox],
            chatbot
        ).then(
            _clear_message,
            outputs=msg
        )
        
        # Effacement de la conversation
        clear_btn.click(_clear_chat, outputs=chatbot)

        print("üåê Lancement de l'interface Gradio...")
        
        # Configuration HTTPS pour Claude Desktop
        ssl_keyfile = os.getenv("SSL_KEYFILE")
        ssl_certfile = os.getenv("SSL_CERTFILE")
        
        if ssl_keyfile and ssl_certfile:
            print("üîí Mode HTTPS activ√©")
            print("üîó Serveur MCP : /gradio_api/mcp/sse")
            
            demo.launch(
                mcp_server=True,  # Toujours activer MCP
                inbrowser=True,
                show_error=True,
                ssl_keyfile=ssl_keyfile,
                ssl_certfile=ssl_certfile
            )
        else:
            print("üîó Serveur MCP : /gradio_api/mcp/sse")
            print("üí° Pour HTTPS : python step03_ssl_generator_optional.py")
            
            demo.launch(
                mcp_server=True,  # Toujours activer MCP
                inbrowser=True,
                show_error=True
            )
        
        print("üìã Outil MCP expos√© : ask_rag_question")

    return 0


if __name__ == "__main__":
    exit(main())